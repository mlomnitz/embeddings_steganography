## Embeddings

This folder should hold the word2vec embeddings. Default performance reported uses:
- Global Vectors for Word Representation [GloVe](https://nlp.stanford.edu/projects/glove/) using 2014 Wikipedia + Gigaword 5 (6B tokens, 400K vocabulary and 50-dim embedings)
- Also see this [post](https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76)
repository uses GLoVE 50 dimensional embeddings.